input {
  kafka {
    sasl_jaas_config => "org.apache.kafka.common.security.scram.ScramLoginModule required username='kafka-connect' password='CQ9GVxZRbFIp';"
    bootstrap_servers => "bootstrap.192.168.55.44.nip.io:443"
    key_deserializer_class => "org.apache.kafka.common.serialization.StringDeserializer"
    value_deserializer_class => "org.apache.kafka.common.serialization.StringDeserializer"
    sasl_mechanism => "SCRAM-SHA-512"
    security_protocol => "SASL_SSL"
    ssl_key_password => "mZ5IDfNdiYl9"
    ssl_keystore_location => "/home/kafka-auth/keystore.jks"
    ssl_keystore_password => "passw0rd"
    ssl_truststore_location => "/home/kafka-auth/truststore.jks"
    ssl_truststore_password => "passw0rd"
    topics => ["hello-cassandra"]
    group_id => "logstash_1"
    client_id => "logstash_1"
  }
}

filter {
  json {
    source => "message"
  }
  mutate { remove_field => [ "message" ] }
}

## Add your filters / logstash plugins configuration here

output {
	elasticsearch {
		hosts => "localhost:9200"
		user => "elastic"
		password => "changeme"
    index => "transactions"
    document_id => "%{[user_id]}"
    action => "update"
    scripted_upsert => true
    script_lang => "painless"
    script_type => "inline"
    script => '
    if (ctx.op == "create") {
      ctx._source = params.event;
    }

    if (ctx.op == "index") {
      ctx._source.amount = params.event.amount;
      ctx._source.type = params.event.type;
      for (d in params.event.dates) {
        ctx._source.dates.add(d);
      }
    }
    '
		ecs_compatibility => disabled
	}
}
